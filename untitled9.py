# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g6Ub8owuW7CasIQRGBj9mHwCLUH-mSbw
"""

# select GPU runtime (Runtime > Change runtime type > GPU)
# then run:

# Basic installs (may need restart during colab)
!pip install -q transformers accelerate datasets peft bitsandbytes scikit-learn matplotlib sentencepiece
# Install trl (optional, useful helpers)
!pip install -q trl

# Check GPU
!nvidia-smi

!pip install -q transformers accelerate datasets peft bitsandbytes scikit-learn matplotlib sentencepiece trl

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv("DiseaseAndSymptoms.csv")
df.head()
df.shape
df

import re


# Ensure columns exist
# Try common alternatives if column names differ
col_map = {}
cols = [c.strip() for c in df.columns]
if 'disease' in [c.lower() for c in cols] and 'symptoms' in [c.lower() for c in cols]:
# map lower-case name to actual
  for c in cols:
    if c.lower() == 'disease': col_map['Disease'] = c
    if c.lower() == 'symptoms': col_map['Symptoms'] = c
else:
# fallback to first two columns
  col_map['Disease'] = cols[0]
  col_map['Symptoms'] = cols[1]


# Rename to standard names
df = df.rename(columns={col_map['Disease']:'Disease', col_map['Symptoms']:'Symptoms'})
# Drop rows with missing disease or symptoms
df = df.dropna(subset=['Disease','Symptoms'])

import html


def clean_disease(name):
  name = str(name).strip().lower()
  name = html.unescape(name)
  # remove stray punctuation but keep hyphens/underscores and spaces and numbers
  name = re.sub(r"[^a-z0-9 \-_]", "", name)
  name = re.sub(r"\s+", " ", name)
  return name
def clean_symptoms(text):
  if pd.isna(text):
    return ""
  s = str(text).lower()
  s = html.unescape(s)
  # unify separators
  s = s.replace(';', ',').replace('/', ',')
  # remove parentheses content (often noise)
  s = re.sub(r"\([^)]*\)", "", s)
  # keep alphanum, commas and spaces
  s = re.sub(r"[^a-z0-9, ]", "", s)
  s = re.sub(r"\s+", " ", s).strip()
  parts = [p.strip() for p in s.split(',') if p.strip()]
  # remove duplicates while preserving order
  seen = set()
  uniq = []
  for p in parts:
    # common normalization: singular/plural naive handling
    p2 = p.rstrip('s') if len(p)>3 else p
    if p2 not in seen:
      seen.add(p2)
      uniq.append(p)
  return ', '.join(uniq)


# apply cleaning
print('before cleaning:', df.shape)
df['Disease'] = df['Disease'].apply(clean_disease)
df['Symptoms'] = df['Symptoms'].apply(clean_symptoms)


# drop empty symptom rows
df = df[df['Symptoms'].str.strip()!='']
# drop duplicates
df = df.drop_duplicates()
print('after cleaning:', df.shape)


# quick sanity checks
print(df['Disease'].nunique(), 'unique diseases')
print(df['Disease'].value_counts().head(20))


# -----------------------
# CELL 5: Handle severe class imbalance (downsample majority)
# -----------------------
# You can choose max_per_class = 100 / 150 / 200 depending on target dataset size.
max_per_class = 200


balanced = df.groupby('Disease').apply(lambda x: x.sample(n=min(len(x), max_per_class), random_state=42)).reset_index(drop=True)
print('balanced shape:', balanced.shape)
print(balanced['Disease'].value_counts().describe())


# Use balanced as final df for training
final_df = balanced.copy()

from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(final_df, test_size=0.2, stratify=final_df['Disease'], random_state=42)
print('train:', train_df.shape, 'test:', test_df.shape)

import json

def make_output(disease):
  return (f"Disease: {disease}\n"
          f"Explanation: These symptoms frequently match {disease} patterns in the dataset.\n"
          f"Note: This is not medical or diagnostic advice. Consult a licensed healthcare professional.")

examples = []
for _, r in final_df.iterrows():
  ex = {
    "instruction": "Identify the disease pattern based on symptoms.",
    "input": r['Symptoms'],
    "output": make_output(r['Disease'])
  }
  examples.append(ex) # Correctly indented

from collections import defaultdict
import random # Ensure random is imported

by_label = defaultdict(list)
for e in examples:
  by_label[e['output'].split('\n')[0].replace('Disease: ','').strip()].append(e)

train_examples = []
test_examples = []
for label, items in by_label.items():
  n = len(items)
  # Ensure k is at least 1 for small classes to avoid empty test sets if a class has only 1 example
  k = max(1, int(n*0.2))
  # shuffle
  random.Random(42).shuffle(items) # Correctly indented
  test_examples.extend(items[:k]) # Correctly indented
  train_examples.extend(items[k:]) # Correctly indented


print('train count', len(train_examples), 'test count', len(test_examples))

with open('train.jsonl','w') as ft:
  for e in train_examples:
    ft.write(json.dumps(e)+"\n")


with open('test.jsonl','w') as ft:
  for e in test_examples:
    ft.write(json.dumps(e)+"\n")


print('Saved train.jsonl and test.jsonl')

!head -n 3 train.jsonl
!head -n 3 test.jsonl

BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

OUTPUT_DIR = 'gemma_lora_disease'

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


print('Loading tokenizer...')
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
if tokenizer.pad_token_id is None:
  tokenizer.pad_token = tokenizer.eos_token


bnb_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_quant_type='nf4',
bnb_4bit_use_double_quant=True,
bnb_4bit_compute_dtype=torch.float16
)


print('Loading model (this may take a minute)')
model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_config, device_map='auto', trust_remote_code=True)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)
from peft import LoraConfig

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

from peft import get_peft_model, prepare_model_for_kbit_training

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

max_length = 512

def format_and_tokenize(example):
    prompt = f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
"""
    full_text = prompt + example['output']
    tokens = tokenizer(full_text, truncation=True, max_length=max_length)
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

max_length = 512

def format_and_tokenize(example):
    prompt = f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
"""
    full_text = prompt + example['output']
    tokens = tokenizer(full_text, truncation=True, max_length=max_length)
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

from datasets import load_dataset

datasets = load_dataset("json", data_files={"train": "train.jsonl", "test": "test.jsonl"})
tokenized = datasets.map(
    format_and_tokenize,
    remove_columns=datasets["train"].column_names
)

from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=2,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=20,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    optim="paged_adamw_8bit",
    report_to="none" # Added to disable wandb logging
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    data_collator=data_collator
)

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)
from peft import LoraConfig

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

print("Removed redundant tokenizer initialization.")

from peft import get_peft_model, prepare_model_for_kbit_training

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

max_length = 512

def format_and_tokenize(example):
    prompt = f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
"""
    full_text = prompt + example['output']
    tokens = tokenizer(full_text, truncation=True, max_length=max_length, padding="max_length") # Added padding="max_length"
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

from datasets import load_dataset

datasets = load_dataset("json", data_files={"train": "train.jsonl", "test": "test.jsonl"})
tokenized = datasets.map(
    format_and_tokenize,
    remove_columns=datasets["train"].column_names
)

small_train = tokenized["train"].select(range(min(32, len(tokenized["train"]))))
trainer_small = Trainer(
    model=model,
    args=training_args.__class__(**{**training_args.to_dict(), "num_train_epochs": 1, "output_dir": "tmp_test_run"}),
    train_dataset=small_train,
    eval_dataset=small_train,
    data_collator=data_collator
)

trainer_small.train()

model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import json
import re
import numpy as np

test_examples = [json.loads(l) for l in open("test.jsonl")]

true = []
pred = []

def build_prompt(inst, inp):
    return f"""### Instruction:
{inst}

### Input:
{inp}

### Response:
"""

def extract_disease(text):
    # Split by the first occurrence of "### Instruction:" if the model over-generates
    text = text.split("### Instruction:")[0].strip()
    # Remove potential "### Response:" prefix if it's still there
    text = text.replace("### Response:", "").strip()

    # Regex to find "Disease: [name]"
    m = re.search(r"Disease:\s*([a-z0-9 _-]+)", text, re.IGNORECASE)
    if m:
        return m.group(1).strip().lower()

    # Fallback if "Disease: [name]" pattern is not found
    # Try to get the first meaningful line as the disease, assuming it's a direct answer
    first_line = text.split('\n')[0].strip().lower()
    if first_line:
        # Remove any stray punctuation and multiple spaces
        first_line = re.sub(r"[^a-z0-9 _-]", "", first_line)
        first_line = re.sub(r"\s+", " ", first_line).strip()
        if first_line: # Ensure it's not empty after cleaning
            return first_line

    return "unparseable_output" # Indicate failure to parse

for ex in test_examples:
    prompt = build_prompt(ex["instruction"], ex["input"])
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Generate output with pad_token_id specified to avoid warning
    output = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)

    # Decode only the generated part
    decoded = tokenizer.decode(output[0, inputs.input_ids.shape[1]:], skip_special_tokens=True)

    true.append(extract_disease(ex["output"])) # Extract true label for consistency
    pred.append(extract_disease(decoded))

# Get all possible labels from both true and predicted values to avoid 'single label' warning
all_labels = sorted(list(set(true + pred)))

# Generate confusion matrix
cm = confusion_matrix(true, pred, labels=all_labels)

# --- Plotting with enhancements ---

# Plot 1: Confusion Matrix (Counts)
fig_counts, ax_counts = plt.subplots(figsize=(15, 15)) # Larger figure size
disp_counts = ConfusionMatrixDisplay(cm, display_labels=all_labels)
disp_counts.plot(ax=ax_counts, xticks_rotation=90, cmap='Blues', values_format='d') # More professional cmap, raw counts
plt.title("Confusion Matrix (Counts)", fontsize=16)
plt.ylabel("True Label", fontsize=14)
plt.xlabel("Predicted Label", fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=10)
fig_counts.tight_layout()
plt.savefig("confusion_matrix_counts.png")
plt.show()

# Plot 2: Normalized Confusion Matrix
fig_norm, ax_norm = plt.subplots(figsize=(15, 15))
# Manually normalize the confusion matrix
cm_normalized = np.array(cm).astype('float') / cm.sum(axis=1)[:, np.newaxis]
# Replace NaNs with 0 for display (occurs if a true label has no predictions)
cm_normalized[np.isnan(cm_normalized)] = 0

disp_norm = ConfusionMatrixDisplay(cm_normalized, display_labels=all_labels)
disp_norm.plot(ax=ax_norm, xticks_rotation=90, cmap='Blues', values_format='.2f') # No 'normalize' arg here
plt.title("Normalized Confusion Matrix", fontsize=16)
plt.ylabel("True Label", fontsize=14)
plt.xlabel("Predicted Label", fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=10)
fig_norm.tight_layout()
plt.savefig("normalized_confusion_matrix.png")
plt.show()

# Print a summary of unparseable outputs if any
unparseable_count_pred = pred.count("unparseable_output")
unparseable_count_true = true.count("unparseable_output")

if unparseable_count_pred > 0:
    print(f"Warning: {unparseable_count_pred} predicted outputs could not be parsed and are marked as 'unparseable_output'.")
if unparseable_count_true > 0:
    print(f"Warning: {unparseable_count_true} true labels resulted in 'unparseable_output' after processing.")

demo_prompt = build_prompt(
    "Identify the disease pattern based on symptoms.",
    "Fever, headache, body pain"
)

inputs = tokenizer(demo_prompt, return_tensors="pt").to(model.device)
out = model.generate(**inputs, max_new_tokens=128)
print(tokenizer.decode(out[0], skip_special_tokens=True))

from google.colab import files
files.download("train.jsonl")
files.download("test.jsonl")
files.download("confusion_matrix.png")
files.download("normalized_confusion_matrix.png")



